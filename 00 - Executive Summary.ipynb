{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Submission\n",
    "---\n",
    "This notebook summarizes the work performed for Project 2.  It walks through the data science process steps as applied to the project and references other notebooks in the repository.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  State Problem\n",
    "---\n",
    "The assignment is to predict housing prices in Ames, Iowa by using a data set provided with approximately 80 featurs.  The model will be scored via a Kaggle competition hosted for the class.  The target to predict is the sales price of homes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather / obtain data\n",
    "---\n",
    "As part of the assignment, data was provided in the following files:\n",
    "* A train file that contains all fields including the SalePrice of the home.\n",
    "* A test file that conatins the same fields as the train file but without the target, SalePrice.\n",
    "* A data dictionary explaining the fields in the train and test file.\n",
    "\n",
    "During my process, I further split the train file into a train2 and test2 so that I could assess the fit of my model.  Later in the process, I used this train2/test2 split to help analyze the residuals of various models to try to find out where my model was weakest and then try to address those instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Explore data\n",
    "---\n",
    "At this point, I direct you to two workbooks in the repo where I performed EDA and data cleaning:\n",
    "* 01 - EDA look at each variable test and train.ipynb\n",
    "* 02 - Data cleaning and additional EDA for Project 2\n",
    "\n",
    "In the first of these notebooks, I explore the data by graphing various aspects -- histograms of each variable and scatter plots of the variables against Sale Price. \n",
    "\n",
    "In the second, I went variable by variable, fixing null values.  The notebook contains details on how I decided to fix each issue.  It also then writes a CSV file out so that other workbooks can use the cleansed files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Model Data\n",
    "---\n",
    "There are two main workbooks where I've modeled the data.  The main workbook to showcase my ability to create models is 03 - Modeling Notebook - Main with all models.  In this work book, I pull in the cleaned data, perform steps needed to select the features for the model (turn some numeric variables into categories; drop certain columns like ID; drop columns not wanted in the model; select certain groups of polynomial varibles, if desired; harmonize the sets of columns in train vs test; etc).   \n",
    "\n",
    "This workbook then shows models for LinearRegression, RidgeCV, LassoCV, and ElasticNetCV.  Before the final workbook was settled, I changed the procedures to use Pipeline and Gridsearch.\n",
    "\n",
    "Lastly, at the bottom of this workbook, I started exploring the sources of variances of two models by graphing their residuals.  This lead to the idea of taking some of one projection versus the other.  \n",
    "\n",
    "The other modeling workbook is 04 - Modeling Notebook - Lots of Lasso runs...  On Friday, I decided to create a number of scenarios, storing the model fit stats in a dictionary and the actual predictions in a dataframe.  I used these to generate some last ditch attempts to better my Kaggle score.  Although nothing came from this.  I was happy that I took a process where I was manually changing variable names to keep track of results to something slightly more automated -- creating a counter that I then used as a label to create dictionary and dataframe entries so that my results could be created and then analyzed later.\n",
    "\n",
    "This data was generated rather manually -- change something in the model (y transfomration, number of columns to include, specific columns to exclude), each run was somewhat manual.  Since I wanted to reference these variables again without recreating the work, I used the pickle library to save the key output (dataframes and dictionary).  I tested that I could pull that infomraiton back up in another workbook before I closed it out.  It was nice to know that at this point, I have learned enough to use non-class resources to solve a problem (we did not cover how to 'pickle' in class).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Evaluate model\n",
    "---\n",
    "The model evaluation occurred frequently and often.  I didn't save the first model that I created (linear regression with a R2 for the test set of -2.98).  But in general, once I got the swing of the model cycle, I was regularly comparing the R2 on the train2 data versus the test2 data (train/test split on the original \"train\" file).  \n",
    "\n",
    "Once I was generating models and not really advancing the model fit, I started to explore the variances generated by the models.  I found that graphing the errors of the test2 data, sorted by the target (Sale Price) was interesting.  It showed that the errors were actually quite normally distributed with a bit of a fanning out as Sale Prices prices got higher.   \n",
    "\n",
    "I was starting to graph two models on the same scatter plot and compare where one model did better than the other, but I didn't really draw many conclusions.  I tried scoring a model where I took the predictions from model 1 if hte prediction was < 130,000 of Sales price and model 2 in all other cases.  This didn't improve my score from Kaggle, and we had just learned of Pipelines.  So, I dropped that effort and went forward with redoing my notebooks to include pipeline workflows.  \n",
    "\n",
    "With averaging, my model fit improved a bit, but I didn't have much luck beyond a modest improvement in my Kaggle score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Answer the question/problem\n",
    "---\n",
    "The scoring mechanism from Kaggle clearly shows that I have completed the assignment:  create a model that predicts housing prices in Ames, Iowa.  \n",
    "\n",
    "sw\n",
    "8/24/18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
